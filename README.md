# Emotion Detection from Voice Signals

## ğŸ“Œ Project Overview
This project is an **AI-powered forensic solution** that detects and categorizes emotions (happiness, sadness, anger, neutrality) from voice inputs. The system supports  **uploaded audio files** while displaying detected emotions as text output.

### ğŸ¯ Key Features
- ğŸ“‚ **Supports both live audio & uploaded files**.
- ğŸ“Š **Uses MFCC feature extraction** for audio processing.
- ğŸ¤– **Deep learning-based classification** with PyTorch.
- ğŸ“ˆ **Scalable & optimized for accuracy** over speed.
- ğŸŒ **Deployed using Flask for API integration.**

## ğŸ› ï¸ Tech Stack
- **Programming Language**: Python ğŸ
- **AI/ML Framework**: PyTorch âš¡
- **Audio Processing**: Librosa ğŸµ
- **Web Framework**: Flask ğŸŒ

## ğŸš€ Installation Guide
### ğŸ”¹ 1. Clone the Repository
```sh
git clone https://github.com/your-repo/emotion-detection.git
cd emotion-detection
```

### ğŸ”¹ 2. Install Dependencies
Ensure Python 3.8+ is installed, then run:
```sh
pip install -r requirements.txt
```
Add these line in the end of App.py

```if __name__ == "__main__":
port = int(os.environ.get("PORT", 5000))  # Get port from Render, default to 5000
app.run(host="0.0.0.0", port=port, debug=True)
```

### ğŸ”¹ 3. Download Pretrained Model
Hugging Fac elink  : https://huggingface.co/sushanthrai/AI_VERCE_HACKTHON/tree/main
Download the `emotion_model.pth` file from hugging face and place them in the project root.
Download the `lable_classes.npy` file from hugging face  and place them in the project root.
And Change the App.py for load model from local


## ğŸ¤ How It Works
### 1ï¸âƒ£ **Feature Extraction (MFCCs)**
- Extracts **32 MFCCs** over **32 frames** from input audio.
- Uses `librosa.feature.mfcc()` for feature extraction.
- The extracted features are **normalized using a scaler**.

### 2ï¸âƒ£ **Deep Learning Model (PyTorch)**
- A **CNN-based model** processes the MFCC features.
- The **Softmax activation** determines the emotion class.
- Outputs **predicted emotion & confidence score**.

### 3ï¸âƒ£ **Flask API for Deployment**
- Provides endpoints for **real-time detection**:
  - `/predict` â†’ Accepts an audio file & returns emotion.
  - `/live` â†’ Captures microphone input for detection.

## ğŸ“Œ Usage Guide
### ğŸ”¹ 1. Run the Flask Server
```sh
python app.py
```
### ğŸ”¹ 2. Test API with an Audio File
```sh
curl -X POST -F "file=@sample.wav" http://127.0.0.1:5000/upload
```
And upload the audio file

Response:
```json
{
  "emotion":"angry",
"message":"File processed successfully"
}
```


## ğŸ“Œ Future Enhancements
- ğŸ™ï¸ Real-time emotion detection from voice input.
- ğŸ§  **Improve model accuracy** using transformer models.
- ğŸ­ **Expand emotion classes** beyond 4 categories.
- ğŸ–¥ï¸ **Integrate with GUI for better UX**.
- â˜ï¸ **Deploy as a cloud-based API** for scalability.

## ğŸ’¡ Contributing
Pull requests are welcome! Feel free to improve the model, API, or add new features.

## ğŸ“ License
MIT License. See `LICENSE` for details.

---
ğŸš€ **Developed by Sushanth Rai & Team** ğŸ¯

